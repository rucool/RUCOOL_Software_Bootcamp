{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# more on Pandas index\n", "\n", "## Credit:\n", "\n", "this comes from Abernathys open book, which we will be looking at a lot! https://earth-env-data-science.github.io/lectures/core_python/python_fundamentals.html\n", "\n", "## Time Indexes\n", "\n", "Indexes are very powerful. They label the data inside a pandas series or dataframe and let you intuitivly work with the data. They are a big part of why Pandas is so useful. There are different indices for different types of data. Time Indexes are especially great!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# dates\n", "Python has a special datatype specific to dates. Dates get treated differently than nummbers or strings, and this feature lets you do lots of powerful timeseries analysis. \n", "\n", "Below we make a special time series using `pd.date_range()` where we can speficy the start, end and frequency of points we want\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["What happened there? We made a range of times (data of a particular type, the pandas `datetime` type) using pandas `pd.date_range()`, then we used numpy (`np.sin()`) to make some fake data based on that time range.\n", "\n", "I added the fake data, and the time range into a pandas `Series` called `timeseries`, which is like one column of a `DataFrame` using the function that creates series: `pd.Series()` , telling the function to used `two_years` as our index for our variable `timeseries`.\n", "\n", "Because we used the special way pandas can create time indicies, when we plotted the `Series` using `timeseries.plot()`, matplotlib knows we are talking about time and labels everything nicely.\n", "\n", "\n", "# indexing and slicing\n", "\n", "Let's say we want to just get some time subset of our data. Pandas has an easy way to let us do that using the `.loc` notation we've seen before:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The TimeIndex object has lots of useful attributes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Reading real Data Files: NOAA Weather Station Data\n", "\n", "In this example, we will use NOAA weather station data from https://www.ncdc.noaa.gov/data-access/land-based-station-data.\n", "\n", "The details of files we are going to read are described in this [README file](ftp://ftp.ncdc.noaa.gov/pub/data/uscrn/products/daily01/README.txt)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have a text file on our hard drive called `data.txt`. **Examine it by opening in jupyterlab.** What do you see?\n", "\n", "To read it into pandas, we will use the [read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function. This function is incredibly complex and powerful. You can use it to extract data from almost any text file. However, you need to understand how to use its various options. Its is awesome, it will let you read data out of almost any text file. However to get the data in a useful format we are going to have to do a few things to it. In the next few steps we will see how to clean and wrangle the data into a good format for use.\n", "\n", "### Cleaning and wrangling messy data into a format you can use is an incredibly important skill to master\n", "\n", "\n", "If we just read the data in with no options, we get something that is a bit of a mess:\n", "\n", "With no options, this is what we get."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Pandas wasn't able to automatically sort out all the columns because, as the name suggests, `pd.read_csv()` expects the data to be in comma separated value (csv) format. Our data is just separated by spaces. \n", "\n", "Fortunatly we can put options into `pd.read_csv()` to tell it what the separation between data is by using the `sep=` keyword. This lets the function read the data correctly. The representation of space is `'\\s+'`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["excellent, much better. now the columns are all separated out well. \n", "\n", "if we looked at all the data we will see there are lots of -99 and -9999 values in the file. If we look closely, we will see there are lots of -99 and -9999 values in the file. The [README file](ftp://ftp.ncdc.noaa.gov/pub/data/uscrn/products/daily01/README.txt) tells us that these are values used to represent missing data. Let's tell this to pandas. We do this using an arguement specification `na_values =[listof bad data values]`. the `na` part stands for NaN (not a number) which will be filled in:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["ok, another good step. Now all the bad data is represented by `NaN`, which is something that pandas and numpy are good at dealing with.\n", "\n", "you can see we are slowing bulding up a good dataframe. We are cleaning out all the warts in the data. This is something you will do over and over!\n", "\n", "Let's check out what is in our Dataframe:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["One problem here is that pandas did not recognize the `LDT_DATE` column as a date. Let's help it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["now we see that the `LST_DATE` column is the special datetime64 type of data that lets pandas do all the cool stuff with time. \n", "\n", "We are one step closer to a good data set!\n", "\n", "the last step: we want to use the date as the index for our dataframe. It's the timestamp that ties all this data together. We can tell pandas to do this too by setting the index of our dataframe to that column"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can take advantage of all the cool time stuff that pandas can do. \n", "\n", "Like let's use indexing to look at all the data from one day"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Or use slicing to get a range:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Quick Statistics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plotting Values\n", "\n", "We can now quickly make plots of the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pandas is very \"time aware\":"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Note: we could also manually create an axis and plot into it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["We can do some statistrical plots too:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Resampling\n", "\n", "Pandas understands how to do all sorts of time related things. It has many methods that let us manipulate timeseries in useful ways. \n", "\n", "For example there is a method called resampling that oranizes the data based on a different time bin than the origional data. \n", "\n", "Let's resample from daily to monthly data:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "The output of the `.resample()` function is an object where the data has been storted into bins where all the data in a bin come from the same month. We then need to do some calculation on that bin to get a vaule. THis is a `groupby` - type operation.\n", "\u200b\n", "we can use this to get monthly-means of all the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["we can chain all these commands together to plot the monthly mean of average, high and low temperatures:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.11"}}, "nbformat": 4, "nbformat_minor": 4}